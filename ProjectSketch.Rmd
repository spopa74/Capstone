Predictive Keyboard. Building the model
=======================================

This markdown file will describe the steps for building the model for the project, written as I go through the tasks. The **main requirement** of this project is **"PREDICTIVE KEYBOARD: ...use the knowledge you gained in data products to build a predictive text product..."**. Basically, based on a pre-constructed predictive model and the current text written, the *predictive keyboard* should propose believable *following* words. 

Deliverables: 
- A predictive text model to test on a real data set
- A reproducible R markdown document describing your model building process (this document)
- A data product built with Shiny or Yhat to demonstrate the use of your product  
 

Introduction to modeling 
------------------------
The project is an exercise in building a predictive model for text input using a keyboard. The predictive model could be a combination of probabilistic models (N-grams, others), and rule-based models (which, in general, could also be modeled using probabilities). For various tasks of the keyboard, different models will be used. 

In general, there are several sources of variability in regards to the possible *following* words. The sources, in short, could be imagined as being the **context** of the respective text (the text currently written, using the keyboard). From the factors defining the context, one could enumerate:  
- **the language** the text is written in. English vs. Russian, etc. The models should be sensitive to the respective language. For this, I will employ a simple rule based model, using the stopwords from the text written, to guess the language. Quick reminder, stopwords are *function words and connectives such as articles and prepositions that appear in a large number of documents... e.g. a, an, the, on for English...*. [2]
- **the application** in which the text will appear. In this case, the 3 possibilities are Twitter, Blogs, News. Certain rules could be used to guess the application (#s combined with short messages for Twitter, unclear about the Blogs and News). But, considering that the text input, in which the keyboard will write, is tightly connected to a specific application (client app for Twitter, Blogs, News), the application could be actually *predefined*. For this project, a button in the UI, which selects between the 3 possibilities, could be employed. 
- **the subject** of the text. Ideally, the keyboard would be able to detect, in general terms, what the text is about (some high frequency words), and based on this information, to *push* some other possible *following words* in front (meaning giving them a higher probability). A probabilistic model involving groups of words, usually used together in texts, could be employed. 
- **the grammar** of the language or of the person. This would be more difficult to model in general (and also would probably fail in case of Twitter or News, which involve some loose grammar or jurnalistic style), but hopefully N-grams will be helpful enough (where N is 4 or 5, but comparisons with shorter N-grams could be also done, and see if they're just as "good"). 
- **the vocabulary** of the user. In this case, we don't really have users (the data given is not associated with users), but the problem is similar to the *subject*, meaning that users tend to use the same words in regards to same subjects. The keyboard, after a while, could, in theory, use the text input to *learn* the user's specifics. 

As a general approach, I'll start by creating a simple N-gram probabilistic model, by putting the texts together, creating the N-grams, and getting a measure of its accuracy. I'll use this model as a baseline. I'll continue by trying some improvements like the ones described above (language selection, subject clustering, user vocabulary learning). A measurement of accuracy will be done here also, and a statistical significance comparison will be attempted (comparison of proportions of accurate selections vs. the length of the text written). 


Task 0 - Understanding the problem
----------------------------------

a). What data do we have?  
- 4 sets of files, containing samples of tweets, blogs and news, in English, German, Finnish and Russian. 
Some basic data, word counts, etc (used *wc* command, recursively). 

```html
  lines       words     file
 371440    12653185    .//de_DE/de_DE.blogs.txt
 244743    13219388    .//de_DE/de_DE.news.txt
 947774    11803735    .//de_DE/de_DE.twitter.txt
 899288    37334690    .//en_US/en_US.blogs.txt
1010242    34372720    .//en_US/en_US.news.txt
2360148    30374206    .//en_US/en_US.twitter.txt
 439785    12732013    .//fi_FI/fi_FI.blogs.txt
 485758    10446725    .//fi_FI/fi_FI.news.txt
 285214     3153003    .//fi_FI/fi_FI.twitter.txt
 337100     9691167    .//ru_RU/ru_RU.blogs.txt
 196360     9416099    .//ru_RU/ru_RU.news.txt
 881414     9542485    .//ru_RU/ru_RU.twitter.txt
```

b). What are the standard tools and models?  
- documentation
    - *Text mining infrastucture in R*: http://www.jstatsoft.org/v25/i05/
    - *CRAN Task View: Natural Language Processing*: http://cran.r-project.org/web/views/NaturalLanguageProcessing.html
    - Coursera course on NLP (not in R): https://class.coursera.org/nlp
    - stemmers, stopwords, various helping info at http://snowball.tartarus.org
    
- libraries
    - *Text Mining* R library, mentioned above
    - *OpenNLP*, Java library for Natural Language Processing
    - *Weka*, open source Java product for various Data Mining and Artificial Intelligence algorithms. The R library mentioned above does integrate with the other two. 

- models
    - the immediate model for predicting following words one could think of is the N-grams: Markov probabilistic model for associating a certain probability to words, depending on the previous (N-1) words. 
    - for selecting the language and/or switching languages, a simple rule based model could be imagined: based on the stopwords typed with the keyboard, decide the language. Allow for changing on the fly, depending on changing the overall count of stopwords in a certain language. Another idea on this line, the model could flip between languages based on the N-grams ("a 4-gram with a different language word at the end maybe should take into consideration 3-, 2-grams instead of an unexisting 4-gram"). 
    - a model based on more and more input from a user should be designed - the users have different "styles" (vocabularies, N-grams, subjects of conversation), so the keyboard should, in general, allow for a user-specific model. This should evolve based on accumulating more input, and the keyboard should behave based on the simple rule "is the N-gram in the user model? If not, use the general model..." (to be determined). 
  
**Questions**  

- *What do the data look like?* The data seems to be text pieces extracted from Tweets, blogs, news.
- *Where do the data come from?* HC Corpora (www.corpora.heliohost.org).
- *Can you think of any other data sources that might help you in this project?*
    - Vocabularies. This would make the parsing of the samples more robust, in the sense that the misspellings would be corrected while parsing the text, and we would get more correct counts for various statistics. 
    - Stemmers, Part of Speech analyzer, Grammars. Working with the stemmer, would be able to extract grammar related rules, and make a more educated guess in regards to the next possible word. 
    - N-grams database from Google. Would work together with the N-grams derived from the data given here. 
    - It would be nice to have the data split "per user". Different users will have different "styles", and also would have a certain set of subjects they discuss about (guess-able from the most frequent words used - after removing the stopwords). The predictive keyboard *could* implement a learner which takes into consideration the previous user's texts, which would work in conjunction with the more general one. 
- *What are the common steps in natural language processing?* 
    - Tokenization: stemming, punctuation, analysis of linked and contracted words ("it's", "-"). 
    - Sentence and structure detection: deal with punctuation. 
    - Part of speech tagging, normalization: resolve ambiguities related to polysemantic words. 
    - Named entity resolution: associate the word to a concept, use ontologies and synonims. 
    - Parsing: find the structure of a sentence. 
    - Building a language model: use N-grams, rules, synonims. Find a representation of the language model.
- *What are some common issues in the analysis of text data?* One could think of: language (each language has its own specifics), problems with stemming (conjugations, declensions), punctuation (ambiguities), grammar (improper), semantic ambiguities, negations.
- *What is the relationship between NLP and the concepts you have learned in the Specialization?* I would say that the most important connection is the statistic/probabilistic treatment of the language models (Markov chains, N-grams).

  
Task 1 - Data acquisition and cleaning
--------------------------------------

**Tasks**

- Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.

```{r}
## While a function for iterating through the lines of text from a file, paying attention to the 
## punctuation, could be imagined, for this one I'll use directly the Weka tokenizer: 
library(RWeka)
tokenize <- function(file) {
    tokens <- c()
    # open file
    lines <- readLines(file)
    # parse lines
    for (i in 1:length(lines)) {
        line <- lines[i]
        tokens <- c(tokens, WordTokenizer(line))
    }
    return(tokens)
}

```

- Profanity filtering - removing profanity and other words you do not want to predict. 
The main idea here is to replace what is being perceived as profanity, with some predefined keyword. A personal choice here, using "..." instead of a profanity. Trivial replacement in the vector of tokens. 

```{r}
filter_profanities <- function(tokens, profanities){
    for (i in 1:length(tokens)){
        if(tokens[i] %in% profanities)
            tokens[i] <- "..."
    }
    return(tokens)
}

```


**Questions**

- *How should you handle punctuation?* Start with a punctuation symbols set, some probabilistic rules to decide "when is really defining structure vs. other usage". The punctuation could help with the parsing (but not necessarily with the N-grams construction. 
- *The data contains lots of times, dates, numbers and currency values. How to handle these? Are they useful for prediction?* Probably not that useful for the exact prediction the following word, but maybe useful for guessing the subject of the text. 
- *How do you find typos in the data?* By comparing with the vocabulary and some rules for stemming. 
- *How do you identify garbage, or the wrong language?* By an *admittedly long* series of unrecognized words. Not sure if one can make a clear distinction between "garbage" and "wrong language", unless we can be confident that we do have a complete set of languages defined in the system - if the text doesn't *belong* in any of the languages, then is *garbage*.
- *How do you define profanity? How do you ensure you don't remove words you want to include?* One could include a list of profanities with variations (some simplistic regular expressions might also work). Not sure a *minimal edit distance* approach would work, since would be very difficult, based on a minimal edit distance approach, to allow words like *duck* or so. 
- *How do you handle capital and lower cases?* Probably the simplest way is to lowercase everything in the model. They could play a role in part of speech analyis (names, etc, which should not be confused with some misspelled word). 
- *What is the best set of features you might use to predict the next word?*
    - N-grams (4- or 5-grams), learned from these files or from the user's input
    - Part of speech analysis (grammar rules for the respective language)


Task 2 - Exploratory analysis
-----------------------------

**Tasks**

- Exploratory analysis  - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora. 
At this point we need a lib to start getting the N-grams out of the corpus. Using tm and RWeka, the code will look like: 
```{r cache=TRUE, eval=FALSE}
## create a tokenizer in RWeka
tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 5))
## load a corpus from a folder
en_texts <- VCorpus(DirSource(directory="data/en_US/small", encoding="UTF-8"), 
                    readerControl=list(language="en"))
## clean up text
en_texts <- tm_map(x=en_texts, FUN=removeNumbers)
en_texts <- tm_map(x=en_texts, FUN=removePunctuation)
en_texts <- tm_map(x=en_texts, FUN=tolower)
## options(mc.cores=1)
## create a term document matrix, with the frequencies
tdm <- TermDocumentMatrix(en_texts, control=list(tokenizer=tokenizer))
```

**Note**: loading in parallel on RWeka/MacOS TermDocumentMatrix crashes with some obscure errors. Suggestion to use option mc.cores=1. 
**Note**: working on a "bigger" TermDocumentMatrix crashes the JVM behind RWeka, OutOfMemory. Suggestion to use option java.parameters="-Xmx4g" (heap size up to 4G)

Some stats: 
```html

size of corpus(lines)        user     system      elapsed
        10,000               32.2        0.3         26.1
       100,000              464.9        6.6        424.5
       
```


 
- Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data. 
 
**Questions**

- *Some words are more frequent than others - what are the distributions of word frequencies?*
- *What are the frequencies of 2-grams and 3-grams in the dataset?*
- *How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?*
- *How do you evaluate how many of the words come from foreign languages?*
- *Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?*


References
-----------

* [1] Natural Language Processing - Dan Jurafsky, Christopher Manning - https://class.coursera.org/nlp/lecture
* [2] Mining the Web, discovering knowledge from hypertext data - Soumen Chakrabarti
* [3] Wikipedia
* [4] Stemming algorithms, stopwords and resources at http://snowball.tartarus.org

